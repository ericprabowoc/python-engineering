{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 20:54:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('kmeans').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 20:54:14 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|           (3,[],[])|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.format('libsvm').load('data/sample_kmeans_data.txt')\n",
    "print(dataset.count(), len(dataset.columns))\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|           (3,[],[])|\n",
      "|(3,[0,1,2],[0.1,0...|\n",
      "|(3,[0,1,2],[0.2,0...|\n",
      "|(3,[0,1,2],[9.0,9...|\n",
      "|(3,[0,1,2],[9.1,9...|\n",
      "|(3,[0,1,2],[9.2,9...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = dataset.select('features')\n",
    "final_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([9.1, 9.1, 9.1]), array([0.05, 0.05, 0.05]), array([0.2, 0.2, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(final_data)\n",
    "centers = model.clusterCenters()\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|           (3,[],[])|         1|\n",
      "|(3,[0,1,2],[0.1,0...|         1|\n",
      "|(3,[0,1,2],[0.2,0...|         2|\n",
      "|(3,[0,1,2],[9.0,9...|         0|\n",
      "|(3,[0,1,2],[9.1,9...|         0|\n",
      "|(3,[0,1,2],[9.2,9...|         0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = model.transform(final_data)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "# def error(point):\n",
    "#     center = model.centers[model.predict(point)]\n",
    "#     return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# WSSSE = final_data.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "# print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53 Clustering Example Code Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/25 20:54:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('cluster').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 7\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+----------------+\n",
      "| area|perimeter|compactness|  length_of_kernel|   width_of_kernel|asymmetry_coefficient|length_of_groove|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+----------------+\n",
      "|15.26|    14.84|      0.871|             5.763|             3.312|                2.221|            5.22|\n",
      "|14.88|    14.57|     0.8811| 5.553999999999999|             3.333|                1.018|           4.956|\n",
      "|14.29|    14.09|      0.905|             5.291|3.3369999999999997|                2.699|           4.825|\n",
      "|13.84|    13.94|     0.8955|             5.324|3.3789999999999996|                2.259|           4.805|\n",
      "|16.14|    14.99|     0.9034|5.6579999999999995|             3.562|                1.355|           5.175|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv('data/seeds_dataset.csv', inferSchema=True, header=True)\n",
    "print(dataset.count(), len(dataset.columns))\n",
    "dataset.show(5)\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+------------------+------------------+---------------------+----------------+--------------------+\n",
      "| area|perimeter|compactness|  length_of_kernel|   width_of_kernel|asymmetry_coefficient|length_of_groove|            features|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+----------------+--------------------+\n",
      "|15.26|    14.84|      0.871|             5.763|             3.312|                2.221|            5.22|[15.26,14.84,0.87...|\n",
      "|14.88|    14.57|     0.8811| 5.553999999999999|             3.333|                1.018|           4.956|[14.88,14.57,0.88...|\n",
      "|14.29|    14.09|      0.905|             5.291|3.3369999999999997|                2.699|           4.825|[14.29,14.09,0.90...|\n",
      "|13.84|    13.94|     0.8955|             5.324|3.3789999999999996|                2.259|           4.805|[13.84,13.94,0.89...|\n",
      "|16.14|    14.99|     0.9034|5.6579999999999995|             3.562|                1.355|           5.175|[16.14,14.99,0.90...|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=dataset.columns, outputCol='features')\n",
    "final_data = assembler.transform(dataset)\n",
    "final_data.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([14.81910448, 14.53716418,  0.88052239,  5.59101493,  3.29935821,\n",
      "        2.70658209,  5.21753731]), array([11.98865854, 13.28439024,  0.85273659,  5.22742683,  2.88008537,\n",
      "        4.58392683,  5.0742439 ]), array([18.72180328, 16.29737705,  0.88508689,  6.20893443,  3.72267213,\n",
      "        3.60359016,  6.06609836])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(final_data)\n",
    "centers = model.clusterCenters()\n",
    "print(centers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22, features=DenseVector([15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22]), scaledFeatures=DenseVector([5.2445, 11.3633, 36.8608, 13.0072, 8.7685, 1.4772, 10.621])),\n",
       " Row(area=14.88, perimeter=14.57, compactness=0.8811, length_of_kernel=5.553999999999999, width_of_kernel=3.333, asymmetry_coefficient=1.018, length_of_groove=4.956, features=DenseVector([14.88, 14.57, 0.8811, 5.554, 3.333, 1.018, 4.956]), scaledFeatures=DenseVector([5.1139, 11.1566, 37.2883, 12.5354, 8.8241, 0.6771, 10.0838])),\n",
       " Row(area=14.29, perimeter=14.09, compactness=0.905, length_of_kernel=5.291, width_of_kernel=3.3369999999999997, asymmetry_coefficient=2.699, length_of_groove=4.825, features=DenseVector([14.29, 14.09, 0.905, 5.291, 3.337, 2.699, 4.825]), scaledFeatures=DenseVector([4.9112, 10.789, 38.2997, 11.9419, 8.8347, 1.7951, 9.8173])),\n",
       " Row(area=13.84, perimeter=13.94, compactness=0.8955, length_of_kernel=5.324, width_of_kernel=3.3789999999999996, asymmetry_coefficient=2.259, length_of_groove=4.805, features=DenseVector([13.84, 13.94, 0.8955, 5.324, 3.379, 2.259, 4.805]), scaledFeatures=DenseVector([4.7565, 10.6742, 37.8977, 12.0163, 8.9459, 1.5024, 9.7766])),\n",
       " Row(area=16.14, perimeter=14.99, compactness=0.9034, length_of_kernel=5.6579999999999995, width_of_kernel=3.562, asymmetry_coefficient=1.355, length_of_groove=5.175, features=DenseVector([16.14, 14.99, 0.9034, 5.658, 3.562, 1.355, 5.175]), scaledFeatures=DenseVector([5.547, 11.4782, 38.232, 12.7702, 9.4304, 0.9012, 10.5294]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "scaler_model = scaler.fit(final_data)\n",
    "scaled_final_data = scaler_model.transform(final_data)\n",
    "scaled_final_data.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428.6082011872446\n",
      "[array([ 6.35645488, 12.40730852, 37.41990178, 13.93860446,  9.7892399 ,\n",
      "        2.41585013, 12.29286107]), array([ 4.96198582, 10.97871333, 37.30930808, 12.44647267,  8.62880781,\n",
      "        1.80061978, 10.41913733]), array([ 4.07497225, 10.14410142, 35.89816849, 11.80812742,  7.54416916,\n",
      "        3.15410901, 10.38031464])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(featuresCol='scaledFeatures', k=3)\n",
    "model = kmeans.fit(scaled_final_data)\n",
    "centers = model.clusterCenters()\n",
    "print(model.summary.trainingCost)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6300001033389961\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         2|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "prediction = model.transform(scaled_final_data)\n",
    "silhouette = evaluator.evaluate(prediction)\n",
    "print(silhouette)\n",
    "prediction.select('prediction').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.709845635070088 656.0328395397873\n",
      "3 0.6300001033389961 428.6082011872446\n",
      "4 0.49894462346666957 370.44428208908664\n",
      "5 0.35860684299522133 330.5785888229312\n",
      "6 0.40117388050151653 288.72206849763154\n",
      "7 0.23492817609483754 276.4910988286958\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|1         |31   |\n",
      "|6         |15   |\n",
      "|3         |45   |\n",
      "|5         |20   |\n",
      "|4         |38   |\n",
      "|2         |36   |\n",
      "|0         |25   |\n",
      "+----------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "for i in range(2,8):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures', k=i)\n",
    "    model = kmeans.fit(scaled_final_data)\n",
    "    centers = model.clusterCenters()\n",
    "    # print(centers)\n",
    "\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    prediction = model.transform(scaled_final_data)\n",
    "    silhouette = evaluator.evaluate(prediction)\n",
    "    print(i, silhouette, model.summary.trainingCost)\n",
    "\n",
    "print(prediction.select('prediction').groupBy('prediction').count().show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7130696287471148 1011.965714423923\n",
      "3 0.6583884755012417 588.7827507911767\n",
      "4 0.5952568216369759 526.4477715556183\n",
      "5 0.5362045150386681 400.6842435662612\n",
      "6 0.5377008391909751 323.8708673531865\n",
      "7 0.5441231096866248 280.5003686112428\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|1         |15   |\n",
      "|6         |33   |\n",
      "|3         |12   |\n",
      "|5         |28   |\n",
      "|4         |25   |\n",
      "|2         |47   |\n",
      "|0         |50   |\n",
      "+----------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "for i in range(2,8):\n",
    "    kmeans = KMeans(featuresCol='features', k=i)\n",
    "    model = kmeans.fit(final_data)\n",
    "    centers = model.clusterCenters()\n",
    "    # print(centers)\n",
    "\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    prediction = model.transform(final_data)\n",
    "    silhouette = evaluator.evaluate(prediction)\n",
    "    print(i, silhouette, model.summary.trainingCost)\n",
    "\n",
    "print(prediction.select('prediction').groupBy('prediction').count().show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
