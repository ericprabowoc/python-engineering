{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/23 10:45:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('logregconsult').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150 16\n",
      "root\n",
      " |-- Call  Failure: integer (nullable = true)\n",
      " |-- Complains: integer (nullable = true)\n",
      " |-- Subscription  Length: integer (nullable = true)\n",
      " |-- Charge  Amount: integer (nullable = true)\n",
      " |-- Seconds of Use: integer (nullable = true)\n",
      " |-- Frequency of use: integer (nullable = true)\n",
      " |-- Frequency of SMS: integer (nullable = true)\n",
      " |-- Distinct Called Numbers: integer (nullable = true)\n",
      " |-- Age Group: integer (nullable = true)\n",
      " |-- Tariff Plan: integer (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Customer Value: double (nullable = true)\n",
      " |-- FN: double (nullable = true)\n",
      " |-- FP: double (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('data/customer_churn.csv', inferSchema=True, header=True)\n",
    "print(data.count(), len(data.columns))\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------------------+--------------+--------------+----------------+----------------+-----------------------+---------+-----------+------+---+--------------+--------+-------+-----+\n",
      "|Call  Failure|Complains|Subscription  Length|Charge  Amount|Seconds of Use|Frequency of use|Frequency of SMS|Distinct Called Numbers|Age Group|Tariff Plan|Status|Age|Customer Value|      FN|     FP|Churn|\n",
      "+-------------+---------+--------------------+--------------+--------------+----------------+----------------+-----------------------+---------+-----------+------+---+--------------+--------+-------+-----+\n",
      "|            8|        0|                  38|             0|          4370|              71|               5|                     17|        3|          1|     1| 30|        197.64| 177.876| 69.764|    0|\n",
      "|            0|        0|                  39|             0|           318|               5|               7|                      4|        2|          1|     2| 25|        46.035| 41.4315|   60.0|    0|\n",
      "|           10|        0|                  37|             0|          2453|              60|             359|                     24|        3|          1|     1| 30|       1536.52|1382.868|203.652|    0|\n",
      "|           10|        0|                  38|             0|          4198|              66|               1|                     35|        1|          1|     1| 15|        240.02| 216.018| 74.002|    0|\n",
      "|            3|        0|                  38|             0|          2393|              58|               2|                     33|        1|          1|     1| 15|       145.805|131.2245|64.5805|    0|\n",
      "+-------------+---------+--------------------+--------------+--------------+----------------+----------------+-----------------------+---------+-----------+------+---+--------------+--------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call  Failure',\n",
       " 'Complains',\n",
       " 'Subscription  Length',\n",
       " 'Charge  Amount',\n",
       " 'Seconds of Use',\n",
       " 'Frequency of use',\n",
       " 'Frequency of SMS',\n",
       " 'Distinct Called Numbers',\n",
       " 'Age Group',\n",
       " 'Tariff Plan',\n",
       " 'Status',\n",
       " 'Age',\n",
       " 'Customer Value',\n",
       " 'FN',\n",
       " 'FP',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209\n",
      "941\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\n",
    "    'Call  Failure',\n",
    "    'Complains',\n",
    "    'Subscription  Length',\n",
    "    'Charge  Amount',\n",
    "    'Seconds of Use',\n",
    "    'Frequency of use',\n",
    "    'Frequency of SMS',\n",
    "    'Distinct Called Numbers',\n",
    "    'Age Group',\n",
    "    'Tariff Plan',\n",
    "    'Status',\n",
    "    'Age',\n",
    "    'Customer Value'], \n",
    "    outputCol='features'\n",
    ")\n",
    "output = assembler.transform(data)\n",
    "final_data = output.select('features', 'churn')\n",
    "train_churn, test_churn = final_data.randomSplit([0.7,0.3])\n",
    "print(train_churn.count())\n",
    "print(test_churn.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_churn = LogisticRegression(labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/23 10:48:47 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/02/23 10:48:47 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "fitted_churn_model = lr_churn.fit(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|               2209|               2209|\n",
      "|   mean|0.16070620190131282|0.10140334993209597|\n",
      "| stddev| 0.3673428989388941|0.30193042078001403|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_sum = fitted_churn_model.summary\n",
    "# training_sum.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13,[1,2,8,9,10,1...|    1|[-3.1664550373954...|[0.04044777772265...|       1.0|\n",
      "|(13,[1,2,8,9,10,1...|    1|[-3.3161101962061...|[0.03502263101725...|       1.0|\n",
      "|(13,[1,2,8,9,10,1...|    1|[-4.1025803261229...|[0.01626117098832...|       1.0|\n",
      "|(13,[1,2,8,9,10,1...|    1|[-4.1823593271527...|[0.01503301519606...|       1.0|\n",
      "|(13,[2,3,8,9,10,1...|    0|[1.67478694091800...|[0.84221300156823...|       0.0|\n",
      "|(13,[2,6,8,9,10,1...|    0|[-0.2625472697719...|[0.43473763628436...|       1.0|\n",
      "|(13,[2,6,8,9,10,1...|    0|[-0.1198847830565...|[0.47006464912345...|       1.0|\n",
      "|(13,[2,6,8,9,10,1...|    1|[0.64803950276190...|[0.65656853320884...|       0.0|\n",
      "|(13,[2,6,8,9,10,1...|    0|[0.52582042736535...|[0.62850776658490...|       0.0|\n",
      "|(13,[2,8,9,10,11]...|    1|[1.06968648738308...|[0.74453728982486...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_and_labels = fitted_churn_model.evaluate(test_churn)\n",
    "pred_and_labels.predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224317817014446\n"
     ]
    }
   ],
   "source": [
    "eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='churn')\n",
    "auc = eval.evaluate(pred_and_labels.predictions)\n",
    "print(auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lr_model = lr_churn.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "670cec61e4f3692aefc152a66589482a97497120e6ec7ba5bd571042f4733341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
